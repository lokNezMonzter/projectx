{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a275eca0bcec1a3",
   "metadata": {},
   "source": [
    "## Example 1: A Demo RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df52dc194f468250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load the documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"../../data\", \n",
    ").load_data(show_progress=True)\n",
    "\n",
    "print(len(documents))\n",
    "\n",
    "# Create the vector store index (in-memory)\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Create the query engine to ask questions\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22425b5809339b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is llama2-chat and what are the variants? Provide citations\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5887d10de2c08af1",
   "metadata": {},
   "source": [
    "## Example 2: Demo RAG With More Control\n",
    "\n",
    "### Step 1: Parse The Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264205c7-6ffc-45a3-b86a-d4eeefe12ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e22b5f-b977-4002-ae6a-60023d6ab54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create the parser\n",
    "parser = LlamaParse(\n",
    "    api_key=os.environ.get(\"LLAMA_CLOUD_API_KEY\"),\n",
    "    result_type=\"markdown\"\n",
    ")\n",
    "\n",
    "# Async load and parse the data\n",
    "documents = await parser.aload_data(\"../../data/docs/lost_in_the_middle.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1d2cd-3a91-4ece-8e75-8dbb62572dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = documents[0].dict()\n",
    "print(document.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc9b44-1042-4563-bed2-86bc25f80e14",
   "metadata": {},
   "source": [
    "### Step 2: Index The Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e222867-6c51-4ee7-ba84-f161345bcf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "Settings.chunk_size = 1000\n",
    "Settings.chunk_overlap = 100\n",
    "\n",
    "# Create the vector index; this is in-memory database\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede3ada1-a483-4bc2-9bbb-a6655c3868f9",
   "metadata": {},
   "source": [
    "### Step 3: Create The Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b509bcc4-cfda-44ba-b8c6-1671237c92e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=4)\n",
    "\n",
    "# Test a sample query\n",
    "response = query_engine.query(\"Explain the lost in the middle problem in less than 500 words. Provide references or citations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b99c40-48ea-4204-bf0f-c47f382356c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the response\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b37e08-f0c5-4afd-9d5f-4536c9df604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the metadata\n",
    "response.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad1e4d2-a1e6-476b-b319-22fddfefbc27",
   "metadata": {},
   "source": [
    "## Example 3: Demo RAG With Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40722a64-5235-4d50-b0f8-24ecb2a5f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import qdrant_client\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c59194-6b50-4495-919d-9bd41569324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the embedding model to use\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9db67d-1d88-4b6c-bc0d-901bfdb0f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e9523-4f8a-4e62-a961-af8d2dab4442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents; llama-index will parse it for us\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"../../data/docs/lost_in_the_middle.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d0cbf0-3b8a-43b9-b81b-3c619e513f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleint to access Qdrant server on Docker instance\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"retrieval_augmented_generation\"\n",
    ")\n",
    "\n",
    "# Create the storage context for local Qdrant (persist data)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Build the vector index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9069d7-d7f7-4f79-b8b4-61d35eaf6cd7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create the query engine and ask questions\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"How does large number of chunks in the retrieval phase affect quality of response?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4169b178-7e99-4976-a500-814fffcb2400",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# View the response\n",
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895821cf-fe22-4450-8163-2ae46600136e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# View the metadata\n",
    "response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baeab77-96d1-4167-bf4f-e65d6e33cb10",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f075a51-b698-454b-b763-7456aff50971",
   "metadata": {},
   "source": [
    "## Example 4: Demo RAG With Qdrant And HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b52393-67bc-4c93-8000-a769f72f224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import qdrant_client\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "885efcc3-6201-4b6d-8407-40763b86e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration settings for embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-base-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70adfc6b-afbe-4490-b40a-3fded1f58972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the documents from directory\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"../../data/docs/lost_in_the_middle.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "677d7b27-8c8f-40ac-85bd-8c3355e5d1ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2342a5cb-c6f2-4ab9-a409-5cebd481b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure qdrant client\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")\n",
    "\n",
    "# Create vector store\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"sample_collection\"\n",
    ")\n",
    "\n",
    "# Configure storage context\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# Build the vector index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context\n",
    ")\n",
    "\n",
    "# Persist the store\n",
    "index.storage_context.persist(\"../../db/qdrant\")\n",
    "\n",
    "# Create the query engine from index\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89a8034d-44c6-43c7-abaf-8cdbedf40e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What are the challenges of large number of chunks in RAG according to the author?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f772d037-7389-492d-ac83-440f946da6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The challenges of a large number of chunks in RAG, according to the author, include the rapid degradation in model performance when models need to reason over information located in the middle of their input context. The author notes that model performance is highest when relevant information is positioned at the very beginning or end of the input context, and suffers degraded performance when forced to utilize information within the middle of the context.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65b45cfa-d924-45a3-8c19-3046d6b222d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'6a08aff2-b4d8-49d7-9f5d-d92378a5e25f': {'page_label': '5',\n",
       "  'file_name': 'lost_in_the_middle.pdf',\n",
       "  'file_path': '../../data/docs/lost_in_the_middle.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 747542,\n",
       "  'creation_date': '2024-07-17',\n",
       "  'last_modified_date': '2024-07-17'},\n",
       " '83f6cfee-29c7-4832-a991-0479375840b5': {'page_label': '3',\n",
       "  'file_name': 'lost_in_the_middle.pdf',\n",
       "  'file_path': '../../data/docs/lost_in_the_middle.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 747542,\n",
       "  'creation_date': '2024-07-17',\n",
       "  'last_modified_date': '2024-07-17'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ca4c0-f3c8-483e-88ff-1b2db1ff0b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
